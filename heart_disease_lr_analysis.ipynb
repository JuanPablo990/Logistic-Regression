{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ddffd7",
   "metadata": {},
   "source": [
    "**Estudiante:** Juan Pablo Nieto Cortes  \n",
    "**Asignatura:** AREP Arquitectura Empresarial\n",
    "\n",
    "# Regresión Logística desde Cero para Predicción de Enfermedad Cardíaca\n",
    "\n",
    "## 1. Introducción y Contexto\n",
    "\n",
    "En este proyecto, implementaremos un modelo de **Regresión Logística** desde cero, utilizando únicamente librerías fundamentales como **NumPy**, **Pandas** y **Matplotlib**. El objetivo es predecir la presencia de enfermedad cardíaca en pacientes basándonos en un conjunto de atributos clínicos.\n",
    "\n",
    "### ¿Qué es la Regresión Logística?\n",
    "La regresión logística es un algoritmo de aprendizaje supervisado utilizado para problemas de **clasificación binaria**. A diferencia de la regresión lineal, que predice valores continuos, la regresión logística estima la probabilidad de que una instancia pertenezca a una clase particular (por ejemplo, \"Enfermo\" vs \"Sano\") utilizando la función sigmoide para mapear las salidas a un rango entre 0 y 1.\n",
    "\n",
    "### Importancia en Medicina\n",
    "En el contexto médico, la capacidad de predecir el riesgo de enfermedad cardíaca (una de las principales causas de muerte a nivel mundial) a partir de datos no invasivos o mínimamente invasivos es crucial. Un modelo bien calibrado puede servir como herramienta de apoyo para el diagnóstico temprano.\n",
    "\n",
    "### Dataset\n",
    "Utilizaremos el dataset de **Predicción de Enfermedad Cardíaca** (disponible en Kaggle/neurocipher), que contiene 270 muestras con 13 atributos clínicos (como edad, presión arterial, colesterol, etc.) y la variable objetivo que indica la presencia o ausencia de enfermedad cardíaca.\n",
    "\n",
    "### Objetivo del Notebook\n",
    "El propósito principal es educativo: entender los fundamentos matemáticos detrás del entrenamiento de modelos. Implementaremos manualmente:\n",
    "- La función de hipótesis (Sigmoide).\n",
    "- La función de costo (Log Loss).\n",
    "- El cálculo de gradientes.\n",
    "- El algoritmo de optimización Gradient Descent.\n",
    "- Regularización L2 para evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d5d19",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploración del Dataset (EDA)\n",
    "\n",
    "Comenzaremos cargando los datos y realizando un análisis exploratorio básico para entender la estructura del dataset, los tipos de datos y la distribución de las clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Ruta al dataset (ajustada a la ubicación de descarga)\n",
    "dataset_path = \"/home/juan/.cache/kagglehub/datasets/neurocipher/heartdisease/versions/1/Heart_Disease_Prediction.csv\"\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Información general\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf60c3",
   "metadata": {},
   "source": [
    "### Análisis de la Variable Objetivo y Distribuciones\n",
    "\n",
    "A continuación, visualizaremos la distribución de la variable objetivo `Heart Disease` para ver si las clases están balanceadas. También observaremos histogramas de algunas variables numéricas clave para entender sus rangos y formas.\n",
    "\n",
    "Es necesario convertir la variable objetivo a formato binario (0 y 1) para la regresión logística. Asumiremos \"Presence\" como 1 y \"Absence\" como 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e33577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir variable objetivo a binaria\n",
    "# Verificamos los valores únicos primero\n",
    "print(\"Valores únicos en 'Heart Disease':\", df['Heart Disease'].unique())\n",
    "\n",
    "df['Target'] = df['Heart Disease'].apply(lambda x: 1 if x == 'Presence' else 0)\n",
    "\n",
    "# 1. Distribución de clases\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['Target'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Distribución de Clases (0: Ausencia, 1: Presencia)')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"Conteo de clases:\")\n",
    "print(df['Target'].value_counts())\n",
    "\n",
    "# 2. Histogramas de variables clave\n",
    "features_to_plot = ['Age', 'BP', 'Cholesterol', 'Max HR']\n",
    "df[features_to_plot].hist(figsize=(10, 8), bins=20, edgecolor='black')\n",
    "plt.suptitle('Histogramas de Variables Numéricas Clave')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec4b75",
   "metadata": {},
   "source": [
    "### Observaciones del EDA\n",
    "- **Balance de clases**: Observamos las barras para determinar si existe un desbalance significativo. Si las clases están relativamente equilibradas, no necesitaremos técnicas complejas de re-muestreo.\n",
    "- **Variables Numéricas**:\n",
    "    - `Age`: Distribución de edades de los pacientes. Es un factor de riesgo importante.\n",
    "    - `BP` (Presión Arterial): Valores altos indican hipertensión.\n",
    "    - `Cholesterol`: Niveles altos son factor de riesgo.\n",
    "    - `Max HR`: Frecuencia cardíaca máxima alcanzada.\n",
    "- **Ausencia de nulos**: `df.info()` nos confirmó si hay datos faltantes que requieran imputación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e15c18",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos\n",
    "\n",
    "La regresión logística entrenada con Gradient Descent es sensible a la escala de las características. Si una variable tiene rangos de 0-1000 (como colesterol) y otra de 0-1 (como sexo), el gradiente oscilará mucho y tardará en converger.\n",
    "\n",
    "Pasos a realizar:\n",
    "1.  **Selección de Features**: Elegiremos al menos 6 variables predictoras relevantes.\n",
    "2.  **Normalización (Z-score)**: Restar la media y dividir por la desviación estándar ($\\frac{x - \\mu}{\\sigma}$).\n",
    "3.  **Split Train/Test Estratificado**: Dividir los datos manteniendo la proporción de clases en ambos conjuntos (70% entrenamiento, 30% prueba).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Selección de Features (al menos 6)\n",
    "# Usaremos: Age, Sex, BP, Cholesterol, Max HR, ST depression\n",
    "selected_features = ['Age', 'Sex', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
    "X_raw = df[selected_features].values\n",
    "y = df['Target'].values\n",
    "\n",
    "print(f\"Features seleccionadas: {selected_features}\")\n",
    "print(f\"Dimensiones X: {X_raw.shape}, y: {y.shape}\")\n",
    "\n",
    "# 2. Separación Train/Test Estratificado Manual\n",
    "# Indices de cada clase\n",
    "idx_0 = np.where(y == 0)[0]\n",
    "idx_1 = np.where(y == 1)[0]\n",
    "\n",
    "# Shuffle indices\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(idx_0)\n",
    "np.random.shuffle(idx_1)\n",
    "\n",
    "# Tamaño de train (70%)\n",
    "split_ratio = 0.7\n",
    "n_train_0 = int(len(idx_0) * split_ratio)\n",
    "n_train_1 = int(len(idx_1) * split_ratio)\n",
    "\n",
    "# Indices train y test\n",
    "train_idx = np.concatenate((idx_0[:n_train_0], idx_1[:n_train_1]))\n",
    "test_idx = np.concatenate((idx_0[n_train_0:], idx_1[n_train_1:]))\n",
    "\n",
    "# Shuffle final para mezclar clases\n",
    "np.random.shuffle(train_idx)\n",
    "np.random.shuffle(test_idx)\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(f\"Train size: {len(X_train_raw)} ({np.mean(y_train):.2f} postive rate)\")\n",
    "print(f\"Test size: {len(X_test_raw)} ({np.mean(y_test):.2f} positive rate)\")\n",
    "\n",
    "# 3. Normalización Manual (Fit en train, Transform en train y test)\n",
    "mean_train = np.mean(X_train_raw, axis=0)\n",
    "std_train = np.std(X_train_raw, axis=0)\n",
    "\n",
    "# Evitar división por cero\n",
    "std_train[std_train == 0] = 1.0\n",
    "\n",
    "X_train_norm = (X_train_raw - mean_train) / std_train\n",
    "X_test_norm = (X_test_raw - mean_train) / std_train\n",
    "\n",
    "# Agregar columna de unos (bias term / intercepto)\n",
    "X_train = np.hstack([np.ones((X_train_norm.shape[0], 1)), X_train_norm])\n",
    "X_test = np.hstack([np.ones((X_test_norm.shape[0], 1)), X_test_norm])\n",
    "\n",
    "print(\"Normalización completada. Se agregó término de bias (columna de 1s).\")\n",
    "print(f\"Forma final X_train: {X_train.shape}\")\n",
    "print(f\"Forma final X_test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e43b41",
   "metadata": {},
   "source": [
    "### Explicación del Preprocesamiento\n",
    "- **Normalización**: Es crítica para Gradient Descent. Al normalizar, la superficie de costo se vuelve más esférica (en lugar de elipses alargadas), permitiendo que el algoritmo converja más rápido y con un paso de aprendizaje ($\\alpha$) más estable.\n",
    "- **Split Estratificado**: Garantiza que tanto el conjunto de entrenamiento como el de prueba sean representativos de la población real. Si hiciéramos un split aleatorio simple en un dataset pequeño y desbalanceado, podríamos acabar con un test set que casi no tenga casos positivos, falseando la evaluación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9b2ff",
   "metadata": {},
   "source": [
    "## 4. Implementación de Logistic Regression desde Cero\n",
    "\n",
    "Implementaremos las funciones nucleares del algoritmo.\n",
    "\n",
    "### Modelo Matemático\n",
    "La hipótesis de la regresión logística está dada por la función sigmoide aplicada a una combinación lineal de los inputs:\n",
    "$$ h_w(x) = \\sigma(w^T x) = \\frac{1}{1 + e^{-(w_0 + w_1 x_1 + ... + w_n x_n)}} $$\n",
    "\n",
    "La función de costo (Log Loss) para $m$ ejemplos es:\n",
    "$$ J(w) = - \\frac{1}{m} \\sum_{i=1}^{m} [ y^{(i)} \\log(h_w(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_w(x^{(i)})) ] $$\n",
    "\n",
    "El gradiente de la función de costo respecto a los pesos es:\n",
    "$$ \\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) x_j^{(i)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Calcula la función sigmoide 1 / (1 + e^-z).\n",
    "    Mayor estabilidad numérica utilizando np.exp(-z) o condicionales si z es muy negativo.\n",
    "    '''\n",
    "    # Clip para evitar overflow/underflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def compute_cost(y, y_hat):\n",
    "    '''\n",
    "    Calcula el costo Log Loss (Cross-Entropy).\n",
    "    y: etiquetas reales (binarias)\n",
    "    y_hat: probabilidades predichas (0 a 1)\n",
    "    '''\n",
    "    m = len(y)\n",
    "    # epsilon pequeño para evitar log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    \n",
    "    cost = - (1/m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return cost\n",
    "\n",
    "def compute_gradients(X, y, y_hat):\n",
    "    '''\n",
    "    Calcula el gradiente dJ/dw.\n",
    "    X: Matriz de features (m, n+1)\n",
    "    y: vector etiquetas reales\n",
    "    y_hat: vector predicciones\n",
    "    '''\n",
    "    m = len(y)\n",
    "    # Error: predicción - real\n",
    "    error = y_hat - y\n",
    "    # Gradiente: (1/m) * X.T dot error\n",
    "    gradients = (1/m) * np.dot(X.T, error)\n",
    "    return gradients\n",
    "\n",
    "def gradient_descent(X, y, alpha, iterations):\n",
    "    '''\n",
    "    Ejecuta el bucle de optimización.\n",
    "    X: Features\n",
    "    y: Labels\n",
    "    alpha: Learning rate\n",
    "    iterations: Número de iteraciones\n",
    "    '''\n",
    "    m, n = X.shape\n",
    "    # Inicializar pesos en ceros\n",
    "    w = np.zeros(n)\n",
    "    cost_history = []\n",
    "    \n",
    "    print(f\"Iniciando entrenamiento con alpha={alpha}, iters={iterations}...\")\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # 1. Forward pass (Cálculo de z y h(x))\n",
    "        z = np.dot(X, w)\n",
    "        y_hat = sigmoid(z)\n",
    "        \n",
    "        # 2. Calcular costo\n",
    "        cost = compute_cost(y, y_hat)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # 3. Calcular gradientes\n",
    "        grads = compute_gradients(X, y, y_hat)\n",
    "        \n",
    "        # 4. Actualizar pesos (w = w - alpha * dw)\n",
    "        w = w - alpha * grads\n",
    "        \n",
    "        # Reporte cada cierto tiempo\n",
    "        if i % (iterations // 10) == 0:\n",
    "            print(f\"Iteración {i}: Costo = {cost:.4f}\")\n",
    "            \n",
    "    return w, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de hiperparámetros\n",
    "ALPHA = 0.1\n",
    "ITERATIONS = 2000\n",
    "\n",
    "# Entrenamiento\n",
    "w_final, cost_history = gradient_descent(X_train, y_train, ALPHA, ITERATIONS)\n",
    "\n",
    "print(f\"\\nPesos finales entrenados: {w_final}\")\n",
    "\n",
    "# Visualización de la convergencia\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(ITERATIONS), cost_history, color='blue')\n",
    "plt.title('Convergencia del Costo (J) vs Iteraciones')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo Log Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec5082",
   "metadata": {},
   "source": [
    "### Análisis del Entrenamiento\n",
    "- **Curva de Costo**: Observamos si la curva desciende suavemente y se aplana (converge). Si el costo oscila o sube, el learning rate ($\\alpha$) podría ser muy alto. Si baja muy lento, podría ser muy bajo.\n",
    "- **Interpretación de Pesos $w$**:\n",
    "    - El peso $w_0$ es el sesgo (intercepto).\n",
    "    - Los pesos $w_1, ..., w_n$ corresponden a cada feature normalizada. La magnitud indica la fuerza de la influencia y el signo indica la dirección (positivo: aumenta riesgo, negativo: disminuye riesgo) relativo a la media.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7db54c",
   "metadata": {},
   "source": [
    "## 5. Predicción y Evaluación\n",
    "\n",
    "Implementaremos la función `predict` usando un umbral de decisión de 0.5. Evaluaremos el modelo usando métricas estándar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación manual de métricas para evitar dependencia de sklearn\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    # [[TN, FP], [FN, TP]]\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[TN, FP], [FN, TP]])\n",
    "\n",
    "def precision_score(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision_score(y_true, y_pred)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "    return 2 * (p * r) / (p + r) if (p + r) > 0 else 0.0\n",
    "\n",
    "\n",
    "def predict(X, w, threshold=0.5):\n",
    "    '''\n",
    "    Predice clases 0 o 1 dado X y pesos w.\n",
    "    '''\n",
    "    z = np.dot(X, w)\n",
    "    probs = sigmoid(z)\n",
    "    predictions = (probs >= threshold).astype(int)\n",
    "    return predictions\n",
    "\n",
    "# Predicciones en Train y Test\n",
    "y_train_pred = predict(X_train, w_final)\n",
    "y_test_pred = predict(X_test, w_final)\n",
    "\n",
    "# Cálculo de métricas\n",
    "metrics_data = {\n",
    "    'Dataset': ['Train', 'Test'],\n",
    "    'Accuracy': [accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)],\n",
    "    'Precision': [precision_score(y_train, y_train_pred), precision_score(y_test, y_test_pred)],\n",
    "    'Recall': [recall_score(y_train, y_train_pred), recall_score(y_test, y_test_pred)],\n",
    "    'F1 Score': [f1_score(y_train, y_train_pred), f1_score(y_test, y_test_pred)]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "display(metrics_df)\n",
    "\n",
    "print(\"\\nMatriz de Confusión (Test):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045b13c",
   "metadata": {},
   "source": [
    "### Interpretación Clínica\n",
    "- **High Recall (Sensibilidad)**: Es deseable en medicina para no dejar pasar casos enfermos (falsos negativos bajos).\n",
    "- **High Precision**: Importante para no alarmar pacientes sanos (falsos positivos bajos).\n",
    "- **Overfitting/Underfitting**: Comparamos métricas de Train vs Test. Si Train >> Test, hay overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbe6c3",
   "metadata": {},
   "source": [
    "## 6. Visualización de Fronteras de Decisión\n",
    "\n",
    "Visualizaremos cómo el modelo separa las clases en un plano 2D. Como tenemos múltiples dimensiones, tomaremos pares de features, entrenaremos un modelo **solo con esos dos features** y graficaremos la línea de separación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_2d(X_2d, y, feature_names):\n",
    "    '''\n",
    "    Entrena un modelo simple en 2D y grafica la frontera.\n",
    "    '''\n",
    "    # Normalizar features locales\n",
    "    mean_2d = np.mean(X_2d, axis=0)\n",
    "    std_2d = np.std(X_2d, axis=0)\n",
    "    X_norm = (X_2d - mean_2d) / std_2d\n",
    "    \n",
    "    # Agregar bias\n",
    "    X_bias = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])\n",
    "    \n",
    "    # Entrenar modelo local\n",
    "    w_2d, _ = gradient_descent(X_bias, y, alpha=0.1, iterations=1000)\n",
    "    \n",
    "    # Crear Grid para plot\n",
    "    x_min, x_max = X_norm[:, 0].min() - 0.5, X_norm[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_norm[:, 1].min() - 0.5, X_norm[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    # Predecir sobre todo el grid\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_bias = np.hstack([np.ones((grid_points.shape[0], 1)), grid_points])\n",
    "    Z = predict(grid_bias, w_2d)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X_norm[:, 0], X_norm[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "    plt.title(f'Frontera de decisión: {feature_names[0]} vs {feature_names[1]}')\n",
    "    plt.xlabel(feature_names[0] + ' (normalizada)')\n",
    "    plt.ylabel(feature_names[1] + ' (normalizada)')\n",
    "    plt.show()\n",
    "\n",
    "# Pares a visualizar\n",
    "pairs = [\n",
    "    ('Age', 'Max HR'),\n",
    "    ('Cholesterol', 'BP'),\n",
    "    ('Age', 'Cholesterol')\n",
    "]\n",
    "\n",
    "# Indices originales de estas features en X_raw/selected_features\n",
    "feat_indices = {name: i for i, name in enumerate(selected_features)}\n",
    "\n",
    "for f1, f2 in pairs:\n",
    "    idx1, idx2 = feat_indices[f1], feat_indices[f2]\n",
    "    X_subset = X_raw[:, [idx1, idx2]] # Usar todos los datos para ilustrar\n",
    "    print(f\"\\nEntrenando y graficando para: {f1} vs {f2}...\")\n",
    "    plot_decision_boundary_2d(X_subset, y, [f1, f2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338866e",
   "metadata": {},
   "source": [
    "### Análisis de Fronteras\n",
    "Observamos que una frontera lineal (recta) puede no ser suficiente para separar perfectamente las clases complejas. Sin embargo, nos da una idea clara de la separabilidad lineal de las variables seleccionadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba7398",
   "metadata": {},
   "source": [
    "## 7. Regularización L2 (Ridge)\n",
    "\n",
    "Implementaremos regularización L2 para penalizar pesos grandes y prevenir overfitting.\n",
    "\n",
    "Nueva función de costo:\n",
    "$$ J(w) = J_{original}(w) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $$\n",
    "(Nota: usualmente no se regulariza el bias $w_0$).\n",
    "\n",
    "Nuevo gradiente:\n",
    "$$ J(w) = J_{original}(w) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc55df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_l2(y, y_hat, w, lambda_reg, m):\n",
    "    # Costo base\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    base_cost = - (1/m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    # Término L2 (excluyendo bias w[0])\n",
    "    l2_term = (lambda_reg / (2 * m)) * np.sum(np.square(w[1:]))\n",
    "    \n",
    "    return base_cost + l2_term\n",
    "\n",
    "def gradient_descent_l2(X, y, alpha, iterations, lambda_reg):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        z = np.dot(X, w)\n",
    "        y_hat = sigmoid(z)\n",
    "        \n",
    "        cost = compute_cost_l2(y, y_hat, w, lambda_reg, m)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        error = y_hat - y\n",
    "        gradients = (1/m) * np.dot(X.T, error)\n",
    "        \n",
    "        # Agregar regularización al gradiente (excepto para w[0])\n",
    "        # w[0] no se penaliza\n",
    "        gradients[1:] += (lambda_reg / m) * w[1:]\n",
    "        \n",
    "        w = w - alpha * gradients\n",
    "        \n",
    "    return w, cost_history\n",
    "\n",
    "# Comparación de lambdas\n",
    "lambdas = [0, 0.1, 1, 10, 50]\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, lam in enumerate(lambdas):\n",
    "    w_l2, costs_l2 = gradient_descent_l2(X_train, y_train, ALPHA, ITERATIONS, lam)\n",
    "    label_name = f'Lambda={lam} (Norma w: {np.linalg.norm(w_l2[1:]):.2f})'\n",
    "    plt.plot(costs_l2, label=label_name, color=colors[i])\n",
    "\n",
    "plt.title('Comparación de Costo con Regularización L2')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo (Regularizado)')\n",
    "plt.legend()\n",
    "plt.ylim(0.3, 0.7) # Zoom para ver diferencias\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b69736",
   "metadata": {},
   "source": [
    "### Impacto de L2\n",
    "Observamos que a medida que aumenta $\\lambda$ (Lambda), la magnitud de los pesos (Norma w) disminuye. Esto reduce la varianza del modelo (reduce overfitting) a costa de agregar algo de sesgo (underfitting si $\\lambda$ es muy grande).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a2b13",
   "metadata": {},
   "source": [
    "## 8. Conclusiones Finales\n",
    "\n",
    "### Aprendizajes Clave\n",
    "1.  **Implementación desde Cero**: Comprender la derivada del gradiente y cómo se actualizan los pesos desmitifica las \"cajas negras\" de librerías como scikit-learn.\n",
    "2.  **Importancia del Preprocesamiento**: Sin normalización, el Gradient Descent convergería muy lentamente o divergiría, especialmente con features de escalas dispares como `Cholesterol` vs `ST depression`.\n",
    "3.  **Regularización**: Vimos cómo penalizar la magnitud de los pesos ayuda a mantener un modelo más simple y generalizable.\n",
    "\n",
    "### Efectividad del Modelo\n",
    "El modelo de regresión logística lineal logró un rendimiento razonable (ver métricas arriba). Dado que es un problema médico, la interpretabilidad de los pesos es una gran ventaja sobre modelos más complejos como Redes Neuronales.\n",
    "\n",
    "### Limitaciones y Trabajo Futuro\n",
    "- **Linealidad**: La regresión logística asume una frontera de decisión lineal. Si las clases no son linealmente separables, el modelo sufre. Se podría mejorar agregando **features polinómicas** (interacciones entre variables).\n",
    "- **Outliers**: El modelo es sensible a outliers. Un análisis más profundo de limpieza de datos podría mejorar métricas.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}