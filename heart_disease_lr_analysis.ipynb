{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ddffd7",
   "metadata": {},
   "source": [
    "**Estudiante:** Juan Pablo Nieto Cortes  \n",
    "**Asignatura:** AREP Arquitectura Empresarial\n",
    "\n",
    "# Regresi\u00f3n Log\u00edstica desde Cero para Predicci\u00f3n de Enfermedad Card\u00edaca\n",
    "\n",
    "## 1. Introducci\u00f3n y Contexto\n",
    "\n",
    "En este proyecto, implementaremos un modelo de **Regresi\u00f3n Log\u00edstica** desde cero, utilizando \u00fanicamente librer\u00edas fundamentales como **NumPy**, **Pandas** y **Matplotlib**. El objetivo es predecir la presencia de enfermedad card\u00edaca en pacientes bas\u00e1ndonos en un conjunto de atributos cl\u00ednicos.\n",
    "\n",
    "### \u00bfQu\u00e9 es la Regresi\u00f3n Log\u00edstica?\n",
    "La regresi\u00f3n log\u00edstica es un algoritmo de aprendizaje supervisado utilizado para problemas de **clasificaci\u00f3n binaria**. A diferencia de la regresi\u00f3n lineal, que predice valores continuos, la regresi\u00f3n log\u00edstica estima la probabilidad de que una instancia pertenezca a una clase particular (por ejemplo, \"Enfermo\" vs \"Sano\") utilizando la funci\u00f3n sigmoide para mapear las salidas a un rango entre 0 y 1.\n",
    "\n",
    "### Importancia en Medicina\n",
    "En el contexto m\u00e9dico, la capacidad de predecir el riesgo de enfermedad card\u00edaca (una de las principales causas de muerte a nivel mundial) a partir de datos no invasivos o m\u00ednimamente invasivos es crucial. Un modelo bien calibrado puede servir como herramienta de apoyo para el diagn\u00f3stico temprano.\n",
    "\n",
    "### Dataset\n",
    "Utilizaremos el dataset de **Predicci\u00f3n de Enfermedad Card\u00edaca** (disponible en Kaggle/neurocipher), que contiene 270 muestras con 13 atributos cl\u00ednicos (como edad, presi\u00f3n arterial, colesterol, etc.) y la variable objetivo que indica la presencia o ausencia de enfermedad card\u00edaca.\n",
    "\n",
    "### Objetivo del Notebook\n",
    "El prop\u00f3sito principal es educativo: entender los fundamentos matem\u00e1ticos detr\u00e1s del entrenamiento de modelos. Implementaremos manualmente:\n",
    "- La funci\u00f3n de hip\u00f3tesis (Sigmoide).\n",
    "- La funci\u00f3n de costo (Log Loss).\n",
    "- El c\u00e1lculo de gradientes.\n",
    "- El algoritmo de optimizaci\u00f3n Gradient Descent.\n",
    "- Regularizaci\u00f3n L2 para evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d5d19",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploraci\u00f3n del Dataset (EDA)\n",
    "\n",
    "Comenzaremos cargando los datos y realizando un an\u00e1lisis exploratorio b\u00e1sico para entender la estructura del dataset, los tipos de datos y la distribuci\u00f3n de las clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4d95b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\Logistic-Regression\\\\Heart_Disease_Prediction.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLogistic-Regression\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHeart_Disease_Prediction.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Cargar dataset\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Mostrar primeras filas\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimeras filas del dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\Logistic-Regression\\\\Heart_Disease_Prediction.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuraci\u00f3n de visualizaci\u00f3n\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Ruta al dataset (ajustada a la ubicaci\u00f3n de descarga)\n",
    "dataset_path = \"Heart_Disease_Prediction.csv\"\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Informaci\u00f3n general\n",
    "print(\"\\nInformaci\u00f3n del dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Estad\u00edsticas descriptivas\n",
    "print(\"\\nEstad\u00edsticas descriptivas:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf60c3",
   "metadata": {},
   "source": [
    "### An\u00e1lisis de la Variable Objetivo y Distribuciones\n",
    "\n",
    "A continuaci\u00f3n, visualizaremos la distribuci\u00f3n de la variable objetivo `Heart Disease` para ver si las clases est\u00e1n balanceadas. Tambi\u00e9n observaremos histogramas de algunas variables num\u00e9ricas clave para entender sus rangos y formas.\n",
    "\n",
    "Es necesario convertir la variable objetivo a formato binario (0 y 1) para la regresi\u00f3n log\u00edstica. Asumiremos \"Presence\" como 1 y \"Absence\" como 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e33577",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convertir variable objetivo a binaria\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Verificamos los valores \u00fanicos primero\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValores \u00fanicos en \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeart Disease\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeart Disease\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeart Disease\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPresence\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1. Distribuci\u00f3n de clases\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Convertir variable objetivo a binaria\n",
    "# Verificamos los valores \u00fanicos primero\n",
    "print(\"Valores \u00fanicos en 'Heart Disease':\", df['Heart Disease'].unique())\n",
    "\n",
    "df['Target'] = df['Heart Disease'].apply(lambda x: 1 if x == 'Presence' else 0)\n",
    "\n",
    "# 1. Distribuci\u00f3n de clases\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['Target'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Distribuci\u00f3n de Clases (0: Ausencia, 1: Presencia)')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"Conteo de clases:\")\n",
    "print(df['Target'].value_counts())\n",
    "\n",
    "# 2. Histogramas de variables clave\n",
    "features_to_plot = ['Age', 'BP', 'Cholesterol', 'Max HR']\n",
    "df[features_to_plot].hist(figsize=(10, 8), bins=20, edgecolor='black')\n",
    "plt.suptitle('Histogramas de Variables Num\u00e9ricas Clave')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec4b75",
   "metadata": {},
   "source": [
    "### Observaciones del EDA\n",
    "- **Balance de clases**: Observamos las barras para determinar si existe un desbalance significativo. Si las clases est\u00e1n relativamente equilibradas, no necesitaremos t\u00e9cnicas complejas de re-muestreo.\n",
    "- **Variables Num\u00e9ricas**:\n",
    "    - `Age`: Distribuci\u00f3n de edades de los pacientes. Es un factor de riesgo importante.\n",
    "    - `BP` (Presi\u00f3n Arterial): Valores altos indican hipertensi\u00f3n.\n",
    "    - `Cholesterol`: Niveles altos son factor de riesgo.\n",
    "    - `Max HR`: Frecuencia card\u00edaca m\u00e1xima alcanzada.\n",
    "- **Ausencia de nulos**: `df.info()` nos confirm\u00f3 si hay datos faltantes que requieran imputaci\u00f3n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e15c18",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos\n",
    "\n",
    "La regresi\u00f3n log\u00edstica entrenada con Gradient Descent es sensible a la escala de las caracter\u00edsticas. Si una variable tiene rangos de 0-1000 (como colesterol) y otra de 0-1 (como sexo), el gradiente oscilar\u00e1 mucho y tardar\u00e1 en converger.\n",
    "\n",
    "Pasos a realizar:\n",
    "1.  **Selecci\u00f3n de Features**: Elegiremos al menos 6 variables predictoras relevantes.\n",
    "2.  **Normalizaci\u00f3n (Z-score)**: Restar la media y dividir por la desviaci\u00f3n est\u00e1ndar ($\\frac{x - \\mu}{\\sigma}$).\n",
    "3.  **Split Train/Test Estratificado**: Dividir los datos manteniendo la proporci\u00f3n de clases en ambos conjuntos (70% entrenamiento, 30% prueba).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Selecci\u00f3n de Features (al menos 6)\n",
    "# Usaremos: Age, Sex, BP, Cholesterol, Max HR, ST depression\n",
    "selected_features = ['Age', 'Sex', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
    "X_raw = df[selected_features].values\n",
    "y = df['Target'].values\n",
    "\n",
    "print(f\"Features seleccionadas: {selected_features}\")\n",
    "print(f\"Dimensiones X: {X_raw.shape}, y: {y.shape}\")\n",
    "\n",
    "# 2. Separaci\u00f3n Train/Test Estratificado Manual\n",
    "# Indices de cada clase\n",
    "idx_0 = np.where(y == 0)[0]\n",
    "idx_1 = np.where(y == 1)[0]\n",
    "\n",
    "# Shuffle indices\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(idx_0)\n",
    "np.random.shuffle(idx_1)\n",
    "\n",
    "# Tama\u00f1o de train (70%)\n",
    "split_ratio = 0.7\n",
    "n_train_0 = int(len(idx_0) * split_ratio)\n",
    "n_train_1 = int(len(idx_1) * split_ratio)\n",
    "\n",
    "# Indices train y test\n",
    "train_idx = np.concatenate((idx_0[:n_train_0], idx_1[:n_train_1]))\n",
    "test_idx = np.concatenate((idx_0[n_train_0:], idx_1[n_train_1:]))\n",
    "\n",
    "# Shuffle final para mezclar clases\n",
    "np.random.shuffle(train_idx)\n",
    "np.random.shuffle(test_idx)\n",
    "\n",
    "X_train_raw = X_raw[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test_raw = X_raw[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(f\"Train size: {len(X_train_raw)} ({np.mean(y_train):.2f} postive rate)\")\n",
    "print(f\"Test size: {len(X_test_raw)} ({np.mean(y_test):.2f} positive rate)\")\n",
    "\n",
    "# 3. Normalizaci\u00f3n Manual (Fit en train, Transform en train y test)\n",
    "mean_train = np.mean(X_train_raw, axis=0)\n",
    "std_train = np.std(X_train_raw, axis=0)\n",
    "\n",
    "# Evitar divisi\u00f3n por cero\n",
    "std_train[std_train == 0] = 1.0\n",
    "\n",
    "X_train_norm = (X_train_raw - mean_train) / std_train\n",
    "X_test_norm = (X_test_raw - mean_train) / std_train\n",
    "\n",
    "# Agregar columna de unos (bias term / intercepto)\n",
    "X_train = np.hstack([np.ones((X_train_norm.shape[0], 1)), X_train_norm])\n",
    "X_test = np.hstack([np.ones((X_test_norm.shape[0], 1)), X_test_norm])\n",
    "\n",
    "print(\"Normalizaci\u00f3n completada. Se agreg\u00f3 t\u00e9rmino de bias (columna de 1s).\")\n",
    "print(f\"Forma final X_train: {X_train.shape}\")\n",
    "print(f\"Forma final X_test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e43b41",
   "metadata": {},
   "source": [
    "### Explicaci\u00f3n del Preprocesamiento\n",
    "- **Normalizaci\u00f3n**: Es cr\u00edtica para Gradient Descent. Al normalizar, la superficie de costo se vuelve m\u00e1s esf\u00e9rica (en lugar de elipses alargadas), permitiendo que el algoritmo converja m\u00e1s r\u00e1pido y con un paso de aprendizaje ($\\alpha$) m\u00e1s estable.\n",
    "- **Split Estratificado**: Garantiza que tanto el conjunto de entrenamiento como el de prueba sean representativos de la poblaci\u00f3n real. Si hici\u00e9ramos un split aleatorio simple en un dataset peque\u00f1o y desbalanceado, podr\u00edamos acabar con un test set que casi no tenga casos positivos, falseando la evaluaci\u00f3n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9b2ff",
   "metadata": {},
   "source": [
    "## 4. Implementaci\u00f3n de Logistic Regression desde Cero\n",
    "\n",
    "Implementaremos las funciones nucleares del algoritmo.\n",
    "\n",
    "### Modelo Matem\u00e1tico\n",
    "La hip\u00f3tesis de la regresi\u00f3n log\u00edstica est\u00e1 dada por la funci\u00f3n sigmoide aplicada a una combinaci\u00f3n lineal de los inputs:\n",
    "$$ h_w(x) = \\sigma(w^T x) = \\frac{1}{1 + e^{-(w_0 + w_1 x_1 + ... + w_n x_n)}} $$\n",
    "\n",
    "La funci\u00f3n de costo (Log Loss) para $m$ ejemplos es:\n",
    "$$ J(w) = - \\frac{1}{m} \\sum_{i=1}^{m} [ y^{(i)} \\log(h_w(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_w(x^{(i)})) ] $$\n",
    "\n",
    "El gradiente de la funci\u00f3n de costo respecto a los pesos es:\n",
    "$$ \\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) x_j^{(i)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Calcula la funci\u00f3n sigmoide 1 / (1 + e^-z).\n",
    "    Mayor estabilidad num\u00e9rica utilizando np.exp(-z) o condicionales si z es muy negativo.\n",
    "    '''\n",
    "    # Clip para evitar overflow/underflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def compute_cost(y, y_hat):\n",
    "    '''\n",
    "    Calcula el costo Log Loss (Cross-Entropy).\n",
    "    y: etiquetas reales (binarias)\n",
    "    y_hat: probabilidades predichas (0 a 1)\n",
    "    '''\n",
    "    m = len(y)\n",
    "    # epsilon peque\u00f1o para evitar log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    \n",
    "    cost = - (1/m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return cost\n",
    "\n",
    "def compute_gradients(X, y, y_hat):\n",
    "    '''\n",
    "    Calcula el gradiente dJ/dw.\n",
    "    X: Matriz de features (m, n+1)\n",
    "    y: vector etiquetas reales\n",
    "    y_hat: vector predicciones\n",
    "    '''\n",
    "    m = len(y)\n",
    "    # Error: predicci\u00f3n - real\n",
    "    error = y_hat - y\n",
    "    # Gradiente: (1/m) * X.T dot error\n",
    "    gradients = (1/m) * np.dot(X.T, error)\n",
    "    return gradients\n",
    "\n",
    "def gradient_descent(X, y, alpha, iterations):\n",
    "    '''\n",
    "    Ejecuta el bucle de optimizaci\u00f3n.\n",
    "    X: Features\n",
    "    y: Labels\n",
    "    alpha: Learning rate\n",
    "    iterations: N\u00famero de iteraciones\n",
    "    '''\n",
    "    m, n = X.shape\n",
    "    # Inicializar pesos en ceros\n",
    "    w = np.zeros(n)\n",
    "    cost_history = []\n",
    "    \n",
    "    print(f\"Iniciando entrenamiento con alpha={alpha}, iters={iterations}...\")\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # 1. Forward pass (C\u00e1lculo de z y h(x))\n",
    "        z = np.dot(X, w)\n",
    "        y_hat = sigmoid(z)\n",
    "        \n",
    "        # 2. Calcular costo\n",
    "        cost = compute_cost(y, y_hat)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # 3. Calcular gradientes\n",
    "        grads = compute_gradients(X, y, y_hat)\n",
    "        \n",
    "        # 4. Actualizar pesos (w = w - alpha * dw)\n",
    "        w = w - alpha * grads\n",
    "        \n",
    "        # Reporte cada cierto tiempo\n",
    "        if i % (iterations // 10) == 0:\n",
    "            print(f\"Iteraci\u00f3n {i}: Costo = {cost:.4f}\")\n",
    "            \n",
    "    return w, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n de hiperpar\u00e1metros\n",
    "ALPHA = 0.1\n",
    "ITERATIONS = 2000\n",
    "\n",
    "# Entrenamiento\n",
    "w_final, cost_history = gradient_descent(X_train, y_train, ALPHA, ITERATIONS)\n",
    "\n",
    "print(f\"\\nPesos finales entrenados: {w_final}\")\n",
    "\n",
    "# Visualizaci\u00f3n de la convergencia\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(ITERATIONS), cost_history, color='blue')\n",
    "plt.title('Convergencia del Costo (J) vs Iteraciones')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo Log Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec5082",
   "metadata": {},
   "source": [
    "### An\u00e1lisis del Entrenamiento\n",
    "- **Curva de Costo**: Observamos si la curva desciende suavemente y se aplana (converge). Si el costo oscila o sube, el learning rate ($\\alpha$) podr\u00eda ser muy alto. Si baja muy lento, podr\u00eda ser muy bajo.\n",
    "- **Interpretaci\u00f3n de Pesos $w$**:\n",
    "    - El peso $w_0$ es el sesgo (intercepto).\n",
    "    - Los pesos $w_1, ..., w_n$ corresponden a cada feature normalizada. La magnitud indica la fuerza de la influencia y el signo indica la direcci\u00f3n (positivo: aumenta riesgo, negativo: disminuye riesgo) relativo a la media.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7db54c",
   "metadata": {},
   "source": [
    "## 5. Predicci\u00f3n y Evaluaci\u00f3n\n",
    "\n",
    "Implementaremos la funci\u00f3n `predict` usando un umbral de decisi\u00f3n de 0.5. Evaluaremos el modelo usando m\u00e9tricas est\u00e1ndar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci\u00f3n manual de m\u00e9tricas para evitar dependencia de sklearn\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    # [[TN, FP], [FN, TP]]\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[TN, FP], [FN, TP]])\n",
    "\n",
    "def precision_score(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision_score(y_true, y_pred)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "    return 2 * (p * r) / (p + r) if (p + r) > 0 else 0.0\n",
    "\n",
    "\n",
    "def predict(X, w, threshold=0.5):\n",
    "    '''\n",
    "    Predice clases 0 o 1 dado X y pesos w.\n",
    "    '''\n",
    "    z = np.dot(X, w)\n",
    "    probs = sigmoid(z)\n",
    "    predictions = (probs >= threshold).astype(int)\n",
    "    return predictions\n",
    "\n",
    "# Predicciones en Train y Test\n",
    "y_train_pred = predict(X_train, w_final)\n",
    "y_test_pred = predict(X_test, w_final)\n",
    "\n",
    "# C\u00e1lculo de m\u00e9tricas\n",
    "metrics_data = {\n",
    "    'Dataset': ['Train', 'Test'],\n",
    "    'Accuracy': [accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)],\n",
    "    'Precision': [precision_score(y_train, y_train_pred), precision_score(y_test, y_test_pred)],\n",
    "    'Recall': [recall_score(y_train, y_train_pred), recall_score(y_test, y_test_pred)],\n",
    "    'F1 Score': [f1_score(y_train, y_train_pred), f1_score(y_test, y_test_pred)]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "display(metrics_df)\n",
    "\n",
    "print(\"\\nMatriz de Confusi\u00f3n (Test):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045b13c",
   "metadata": {},
   "source": [
    "### Interpretaci\u00f3n Cl\u00ednica\n",
    "- **High Recall (Sensibilidad)**: Es deseable en medicina para no dejar pasar casos enfermos (falsos negativos bajos).\n",
    "- **High Precision**: Importante para no alarmar pacientes sanos (falsos positivos bajos).\n",
    "- **Overfitting/Underfitting**: Comparamos m\u00e9tricas de Train vs Test. Si Train >> Test, hay overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbe6c3",
   "metadata": {},
   "source": [
    "## 6. Visualizaci\u00f3n de Fronteras de Decisi\u00f3n\n",
    "\n",
    "Visualizaremos c\u00f3mo el modelo separa las clases en un plano 2D. Como tenemos m\u00faltiples dimensiones, tomaremos pares de features, entrenaremos un modelo **solo con esos dos features** y graficaremos la l\u00ednea de separaci\u00f3n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_2d(X_2d, y, feature_names):\n",
    "    '''\n",
    "    Entrena un modelo simple en 2D y grafica la frontera.\n",
    "    '''\n",
    "    # Normalizar features locales\n",
    "    mean_2d = np.mean(X_2d, axis=0)\n",
    "    std_2d = np.std(X_2d, axis=0)\n",
    "    X_norm = (X_2d - mean_2d) / std_2d\n",
    "    \n",
    "    # Agregar bias\n",
    "    X_bias = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])\n",
    "    \n",
    "    # Entrenar modelo local\n",
    "    w_2d, _ = gradient_descent(X_bias, y, alpha=0.1, iterations=1000)\n",
    "    \n",
    "    # Crear Grid para plot\n",
    "    x_min, x_max = X_norm[:, 0].min() - 0.5, X_norm[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_norm[:, 1].min() - 0.5, X_norm[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    # Predecir sobre todo el grid\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_bias = np.hstack([np.ones((grid_points.shape[0], 1)), grid_points])\n",
    "    Z = predict(grid_bias, w_2d)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X_norm[:, 0], X_norm[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "    plt.title(f'Frontera de decisi\u00f3n: {feature_names[0]} vs {feature_names[1]}')\n",
    "    plt.xlabel(feature_names[0] + ' (normalizada)')\n",
    "    plt.ylabel(feature_names[1] + ' (normalizada)')\n",
    "    plt.show()\n",
    "\n",
    "# Pares a visualizar\n",
    "pairs = [\n",
    "    ('Age', 'Max HR'),\n",
    "    ('Cholesterol', 'BP'),\n",
    "    ('Age', 'Cholesterol')\n",
    "]\n",
    "\n",
    "# Indices originales de estas features en X_raw/selected_features\n",
    "feat_indices = {name: i for i, name in enumerate(selected_features)}\n",
    "\n",
    "for f1, f2 in pairs:\n",
    "    idx1, idx2 = feat_indices[f1], feat_indices[f2]\n",
    "    X_subset = X_raw[:, [idx1, idx2]] # Usar todos los datos para ilustrar\n",
    "    print(f\"\\nEntrenando y graficando para: {f1} vs {f2}...\")\n",
    "    plot_decision_boundary_2d(X_subset, y, [f1, f2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338866e",
   "metadata": {},
   "source": [
    "### An\u00e1lisis de Fronteras\n",
    "Observamos que una frontera lineal (recta) puede no ser suficiente para separar perfectamente las clases complejas. Sin embargo, nos da una idea clara de la separabilidad lineal de las variables seleccionadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba7398",
   "metadata": {},
   "source": [
    "## 7. Regularizaci\u00f3n L2 (Ridge)\n",
    "\n",
    "Implementaremos regularizaci\u00f3n L2 para penalizar pesos grandes y prevenir overfitting.\n",
    "\n",
    "Nueva funci\u00f3n de costo:\n",
    "$$ J(w) = J_{original}(w) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $$\n",
    "(Nota: usualmente no se regulariza el bias $w_0$).\n",
    "\n",
    "Nuevo gradiente:\n",
    "$$ J(w) = J_{original}(w) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc55df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_l2(y, y_hat, w, lambda_reg, m):\n",
    "    # Costo base\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    base_cost = - (1/m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    # T\u00e9rmino L2 (excluyendo bias w[0])\n",
    "    l2_term = (lambda_reg / (2 * m)) * np.sum(np.square(w[1:]))\n",
    "    \n",
    "    return base_cost + l2_term\n",
    "\n",
    "def gradient_descent_l2(X, y, alpha, iterations, lambda_reg):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        z = np.dot(X, w)\n",
    "        y_hat = sigmoid(z)\n",
    "        \n",
    "        cost = compute_cost_l2(y, y_hat, w, lambda_reg, m)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        error = y_hat - y\n",
    "        gradients = (1/m) * np.dot(X.T, error)\n",
    "        \n",
    "        # Agregar regularizaci\u00f3n al gradiente (excepto para w[0])\n",
    "        # w[0] no se penaliza\n",
    "        gradients[1:] += (lambda_reg / m) * w[1:]\n",
    "        \n",
    "        w = w - alpha * gradients\n",
    "        \n",
    "    return w, cost_history\n",
    "\n",
    "# Comparaci\u00f3n de lambdas\n",
    "lambdas = [0, 0.1, 1, 10, 50]\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, lam in enumerate(lambdas):\n",
    "    w_l2, costs_l2 = gradient_descent_l2(X_train, y_train, ALPHA, ITERATIONS, lam)\n",
    "    label_name = f'Lambda={lam} (Norma w: {np.linalg.norm(w_l2[1:]):.2f})'\n",
    "    plt.plot(costs_l2, label=label_name, color=colors[i])\n",
    "\n",
    "plt.title('Comparaci\u00f3n de Costo con Regularizaci\u00f3n L2')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo (Regularizado)')\n",
    "plt.legend()\n",
    "plt.ylim(0.3, 0.7) # Zoom para ver diferencias\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b69736",
   "metadata": {},
   "source": [
    "### Impacto de L2\n",
    "Observamos que a medida que aumenta $\\lambda$ (Lambda), la magnitud de los pesos (Norma w) disminuye. Esto reduce la varianza del modelo (reduce overfitting) a costa de agregar algo de sesgo (underfitting si $\\lambda$ es muy grande).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a2b13",
   "metadata": {},
   "source": [
    "## 8. Conclusiones Finales\n",
    "\n",
    "### Aprendizajes Clave\n",
    "1.  **Implementaci\u00f3n desde Cero**: Comprender la derivada del gradiente y c\u00f3mo se actualizan los pesos desmitifica las \"cajas negras\" de librer\u00edas como scikit-learn.\n",
    "2.  **Importancia del Preprocesamiento**: Sin normalizaci\u00f3n, el Gradient Descent converger\u00eda muy lentamente o divergir\u00eda, especialmente con features de escalas dispares como `Cholesterol` vs `ST depression`.\n",
    "3.  **Regularizaci\u00f3n**: Vimos c\u00f3mo penalizar la magnitud de los pesos ayuda a mantener un modelo m\u00e1s simple y generalizable.\n",
    "\n",
    "### Efectividad del Modelo\n",
    "El modelo de regresi\u00f3n log\u00edstica lineal logr\u00f3 un rendimiento razonable (ver m\u00e9tricas arriba). Dado que es un problema m\u00e9dico, la interpretabilidad de los pesos es una gran ventaja sobre modelos m\u00e1s complejos como Redes Neuronales.\n",
    "\n",
    "### Limitaciones y Trabajo Futuro\n",
    "- **Linealidad**: La regresi\u00f3n log\u00edstica asume una frontera de decisi\u00f3n lineal. Si las clases no son linealmente separables, el modelo sufre. Se podr\u00eda mejorar agregando **features polin\u00f3micas** (interacciones entre variables).\n",
    "- **Outliers**: El modelo es sensible a outliers. Un an\u00e1lisis m\u00e1s profundo de limpieza de datos podr\u00eda mejorar m\u00e9tricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Preparaci\u00f3n para SageMaker\n",
    "\n",
    "Para desplegar este modelo en Amazon SageMaker, necesitamos:\n",
    "1. Guardar los pesos entrenados (`w_final`).\n",
    "2. Crear un script de inferencia (`inference.py`) que le diga a SageMaker c\u00f3mo cargar los pesos y hacer predicciones.\n",
    "3. Empaquetar todo en un archivo `model.tar.gz`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "# 1. Guardar los pesos del modelo\n",
    "np.save('model_weights.npy', w_final)\n",
    "print(\"Pesos guardados en 'model_weights.npy'\")\n",
    "\n",
    "# 2. Crear script de inferencia (inference.py)\n",
    "inference_code = \"\"\"\nimport numpy as np\nimport json\nimport os\n\ndef model_fn(model_dir):\n    # Carga el modelo desde el directorio de artefactos\n    print(\"Cargando modelo desde: \" + model_dir)\n    weights = np.load(os.path.join(model_dir, \"model_weights.npy\"))\n    return weights\n\ndef input_fn(request_body, request_content_type):\n    # Procesa la entrada. Espera JSON.\n    if request_content_type == 'application/json':\n        input_data = json.loads(request_body)\n        return input_data\n    else:\n        raise ValueError(\"Este modelo solo soporta application/json\")\n\ndef predict_fn(input_data, model):\n    # Realiza la predicci\u00f3n\n    # model es el vector de pesos w_final\n    data = np.array(input_data)\n    \n    # Manejo de dimensiones\n    if data.ndim == 1:\n        data = data.reshape(1, -1)\n        \n    # Agregar Sesgo (Bias) - Columna de 1s al inicio\n    # Asumimos que el cliente env\u00eda clean features sin bias\n    m = data.shape[0]\n    data_bias = np.hstack([np.ones((m, 1)), data])\n    \n    # Funci\u00f3n Sigmoide\n    z = np.dot(data_bias, model)\n    # Clip para estabilidad\n    z = np.clip(z, -500, 500)\n    prob = 1.0 / (1.0 + np.exp(-z))\n    \n    return prob\n\ndef output_fn(prediction, response_content_type):\n    # Formatea la salida\n    return json.dumps(prediction.tolist())\n\"\"\"\n",
    "\n",
    "with open('inference.py', 'w') as f:\n",
    "    f.write(inference_code)\n",
    "print(\"Script 'inference.py' creado.\")\n",
    "\n",
    "# 3. Crear model.tar.gz\n",
    "with tarfile.open('model.tar.gz', \"w:gz\") as tar:\n",
    "    tar.add('model_weights.npy', arcname='model_weights.npy')\n",
    "    tar.add('inference.py', arcname='inference.py')\n",
    "\n",
    "print(\"Archivo 'model.tar.gz' creado exitosamente. Este archivo se sube a S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Despliegue Autom\u00e1tico con SageMaker SDK\n",
    "\n",
    "Si prefieres no usar la consola manual, puedes ejecutar el despliegue directamente desde aqu\u00ed usando el SDK de Python de SageMaker (`sagemaker`).\n",
    "\n",
    "**Requisitos Previos**:\n",
    "1.  Tener una cuenta de AWS y credenciales configuradas en tu m\u00e1quina (archivo `~/.aws/credentials` o variables de entorno).\n",
    "2.  Tener un **IAM Role** en AWS con permisos de `AmazonSageMakerFullAccess` y acceso a S3.\n",
    "3.  Instalar librer\u00edas de AWS: `pip install sagemaker boto3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import Model\n",
    "\n",
    "# --- CONFIGURACI\u00d3N ---\n",
    "# REEMPLAZA ESTO con el ARN de tu rol de IAM. Ejemplo: 'arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole-2020...'\n",
    "role_arn = \"arn:aws:iam::TU_CUENTA_ID:role/TU_ROL_SAGEMAKER\" \n",
    "\n",
    "if role_arn == \"arn:aws:iam::TU_CUENTA_ID:role/TU_ROL_SAGEMAKER\":\n",
    "    print(\"\u26a0\ufe0f DEBES ACTUALIZAR LA VARIABLE 'role_arn' CON TU ROL DE AWS ANTES DE CONTINUAR\")\n",
    "else:\n",
    "    # Sesi\u00f3n y Bucket\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    region = sess.boto_region_name\n",
    "    prefix = 'heart-disease-lr'\n",
    "    \n",
    "    # 1. Subir model.tar.gz a S3\n",
    "    model_artifact = sess.upload_data(path='model.tar.gz', bucket=bucket, key_prefix=prefix)\n",
    "    print(f\"Artefacto subido a: {model_artifact}\")\n",
    "    \n",
    "    # 2. Definir imagen del contenedor (Usamos sklearn como base ligera)\n",
    "    # Recuperamos la URI de la imagen pre-construida de AWS\n",
    "    image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"sklearn\",\n",
    "        region=region,\n",
    "        version=\"1.0-1\",\n",
    "        image_scope=\"inference\"\n",
    "    )\n",
    "    \n",
    "    # 3. Crear Objeto Modelo\n",
    "    model = Model(\n",
    "        image_uri=image_uri,\n",
    "        model_data=model_artifact,\n",
    "        role=role_arn,\n",
    "        sagemaker_session=sess,\n",
    "        name=\"heart-disease-custom-lr\"\n",
    "    )\n",
    "    \n",
    "    # 4. Desplegar Endpoint\n",
    "    print(\"Desplegando endpoint... (esto toma 5-10 mins)\")\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.t2.medium', # Instancia econ\u00f3mica\n",
    "        endpoint_name='heart-disease-endpoint-manual'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEndpoint desplegado: {predictor.endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Simulaci\u00f3n de Despliegue Local (Prueba de Concepto)\n",
    "\n",
    "Antes de subir a la nube, es vital probar que nuestros scripts (`inference.py`) y artefactos (`model.tar.gz`) funcionan correctamente. Esta secci\u00f3n simula el entorno de SageMaker ejecutando todo **localmente en tu m\u00e1quina**.\n",
    "\n",
    "**\u00bfQu\u00e9 hace este c\u00f3digo?**\n",
    "1. Descomprime el archivo `model.tar.gz` (simulando que el servidor lo descarg\u00f3 de S3).\n",
    "2. Importa din\u00e1micamente tu script `inference.py`.\n",
    "3. Ejecuta la funci\u00f3n `model_fn` para cargar el modelo.\n",
    "4. Ejecuta `predict_fn` con datos de prueba para ver si funciona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Simular entorno: Crear carpeta temporal para descomprimir\n",
    "local_deploy_dir = 'local_deployment'\n",
    "if not os.path.exists(local_deploy_dir):\n",
    "    os.makedirs(local_deploy_dir)\n",
    "\n",
    "print(f\"Descomprimiendo model.tar.gz en {local_deploy_dir}...\")\n",
    "with tarfile.open('model.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path=local_deploy_dir)\n",
    "\n",
    "# 2. Cargar din\u00e1micamente el script inference.py\n",
    "inference_script_path = os.path.join(local_deploy_dir, 'inference.py')\n",
    "spec = importlib.util.spec_from_file_location(\"inference\", inference_script_path)\n",
    "inference = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"inference\"] = inference\n",
    "spec.loader.exec_module(inference)\n",
    "print(\"Script inference.py cargado exitosamente como m\u00f3dulo.\")\n",
    "\n",
    "# 3. Ejecutar el flujo de inferencia\n",
    "print(\"\\n--- Iniciando Prueba de Inferencia ---\")\n",
    "\n",
    "# A. Cargar Modelo\n",
    "model = inference.model_fn(local_deploy_dir)\n",
    "print(f\"Modelo cargado. Pesos: {model[:5]}... (truncado)\")\n",
    "\n",
    "# B. Datos de prueba (Un paciente ficticio)\n",
    "# Normalizamos igual que en el training (usando media/std de ejemplo o 0 para simplificar)\n",
    "# Ejemplo: [Age, Sex, BP, Cholesterol, Max HR, ST depression] (valores normalizados)\n",
    "test_input = [0.5, 1.0, 0.2, -0.5, -1.0, 2.0]\n",
    "print(f\"Input (paciente): {test_input}\")\n",
    "\n",
    "# C. Predecir\n",
    "prediction = inference.predict_fn(test_input, model)\n",
    "print(f\"Probabilidad predicha: {prediction.item():.4f}\")\n",
    "\n",
    "# D. Formato de Salida\n",
    "output = inference.output_fn(prediction, 'application/json')\n",
    "print(f\"Respuesta JSON final: {output}\")\n",
    "\n",
    "print(\"\\n\u2705 \u00a1El despliegue local funciona correctamente! El modelo est\u00e1 listo para AWS.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}